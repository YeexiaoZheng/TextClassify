{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造全局词典，并将词语转为数字生成不同于MLP处理方法的词向量\n",
    "该词向量仅以最长的text词语个数为最大维度，将每个单词以在vocab中的索引做替换，以0填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [03:01<00:00, 44.17it/s]\n",
      "100%|██████████| 8000/8000 [00:00<00:00, 12269.26it/s]\n",
      "100%|██████████| 8000/8000 [00:01<00:00, 7746.34it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('./exp1data/train_data.txt') as f:\n",
    "    train_data_raw = f.read()\n",
    "    f.close()\n",
    "\n",
    "train_data_raw = [json.loads(data) for data in train_data_raw.strip().split('\\n')]\n",
    "\n",
    "word_set = set()\n",
    "for data in tqdm(train_data_raw):\n",
    "    data['nlp']=nlp(data['raw'])\n",
    "    for token in data['nlp']:\n",
    "        if (not token.is_stop) and (not token.is_punct):\n",
    "            word_set.add(token.lemma_)\n",
    "\n",
    "vocab = list(word_set)\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "max_word_size = 0\n",
    "for data in tqdm(train_data_raw):\n",
    "    temp_size = 0\n",
    "    for token in data['nlp']:\n",
    "        if token.lemma_ in word_set:\n",
    "            temp_size += 1\n",
    "    if temp_size > max_word_size:\n",
    "        max_word_size = temp_size\n",
    "\n",
    "for data in tqdm(train_data_raw):\n",
    "    vec = np.zeros(max_word_size)\n",
    "    idx = 0\n",
    "    for token in data['nlp']:\n",
    "        try:\n",
    "            vec[idx] = word2idx[token.lemma_]\n",
    "            idx += 1\n",
    "        except:\n",
    "            pass\n",
    "    data['vec'] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:00<00:00, 36123.34it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "train_label = []\n",
    "\n",
    "for data in tqdm(train_data_raw):\n",
    "    train_data.append(list(data['vec']))\n",
    "    train_label.append(data['label'])\n",
    "\n",
    "all_data = np.array(train_data)\n",
    "all_label = np.array(train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分割训练/验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data, train_label, val_label = train_test_split(all_data, all_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入Pytorch包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "PRINT = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将ndarray转为tensor格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(\n",
    "    torch.LongTensor(train_data),\n",
    "    torch.LongTensor(train_label)\n",
    "    )\n",
    "\n",
    "loader_train = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(\n",
    "    torch.LongTensor(val_data),\n",
    "    torch.LongTensor(val_label)\n",
    "    )\n",
    "loader_val = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab, embedding_size, max_word, class_num):\n",
    "        super(TextCNN, self).__init__()\n",
    "\n",
    "        # Embedding\n",
    "        self.ebd = nn.Embedding(len(vocab), embedding_size)\n",
    "\n",
    "        # CNN\n",
    "        output_channel = 100          # 这里设置多通道防止过拟合，但是在原论文的实际实验中效果不大\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=output_channel, kernel_size=(2, embedding_size), stride=1),    # 经过这一卷积层后feature map大小变为(maxword-1) * embedding_size\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),   # 经过maxpooling后变为(maxword-1)/2 * embedding_size\n",
    "            nn.Dropout(0.5)         # 同样为了防止过拟合\n",
    "        )\n",
    "\n",
    "        # Fully Connected\n",
    "        self.fc = nn.Linear(int((max_word - 1) / 2) * output_channel, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.ebd(x).unsqueeze(1)\n",
    "        conved = self.cnn(embedding)\n",
    "        flatten = conved.view(x.shape[0], -1)\n",
    "        output = self.fc(flatten)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造训练及验证函数（同MLP）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(loader_val, model):\n",
    "    model.eval()\n",
    "    cor, all = 0, 0\n",
    "    for (x, y) in loader_val:\n",
    "        all += len(y)\n",
    "        scores = model(x)\n",
    "        for idx, each in enumerate(scores):\n",
    "            if y[idx] == np.argmax(each.detach().numpy()): \n",
    "                cor += 1\n",
    "\n",
    "    acc = cor / all\n",
    "    print('val acc: ', acc)\n",
    "\n",
    "def train(model, loss_func, optim, loader_train, loader_val, epoch=1):\n",
    "    for e in range(epoch):\n",
    "        for idx, (x, y) in enumerate(loader_train):\n",
    "            # switch to train mode\n",
    "            model.train()\n",
    "            scores = model(x)\n",
    "            loss = loss_func(scores, y)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            if idx % PRINT == 0:\n",
    "                print('Epoch %d, Iteration %d, loss = %.4f' % (e, idx, loss.item()))\n",
    "                if loader_val:\n",
    "                    val(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "此处调参过程已省略，具体调参过程参照实验报告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0, loss = 2.3675\n",
      "val acc:  0.09\n",
      "\n",
      "Epoch 0, Iteration 50, loss = 2.0775\n",
      "val acc:  0.355\n",
      "\n",
      "Epoch 1, Iteration 0, loss = 0.9228\n",
      "val acc:  0.645625\n",
      "\n",
      "Epoch 1, Iteration 50, loss = 0.8427\n",
      "val acc:  0.72875\n",
      "\n",
      "Epoch 2, Iteration 0, loss = 0.5205\n",
      "val acc:  0.773125\n",
      "\n",
      "Epoch 2, Iteration 50, loss = 0.4548\n",
      "val acc:  0.79125\n",
      "\n",
      "Epoch 3, Iteration 0, loss = 0.2167\n",
      "val acc:  0.81625\n",
      "\n",
      "Epoch 3, Iteration 50, loss = 0.3069\n",
      "val acc:  0.82125\n",
      "\n",
      "Epoch 4, Iteration 0, loss = 0.1416\n",
      "val acc:  0.82875\n",
      "\n",
      "Epoch 4, Iteration 50, loss = 0.1170\n",
      "val acc:  0.823125\n",
      "\n",
      "Epoch 5, Iteration 0, loss = 0.0812\n",
      "val acc:  0.83875\n",
      "\n",
      "Epoch 5, Iteration 50, loss = 0.1363\n",
      "val acc:  0.8425\n",
      "\n",
      "Epoch 6, Iteration 0, loss = 0.0447\n",
      "val acc:  0.84875\n",
      "\n",
      "Epoch 6, Iteration 50, loss = 0.1050\n",
      "val acc:  0.8575\n",
      "\n",
      "Epoch 7, Iteration 0, loss = 0.1041\n",
      "val acc:  0.86375\n",
      "\n",
      "Epoch 7, Iteration 50, loss = 0.0665\n",
      "val acc:  0.865625\n",
      "\n",
      "Epoch 8, Iteration 0, loss = 0.0678\n",
      "val acc:  0.87625\n",
      "\n",
      "Epoch 8, Iteration 50, loss = 0.0793\n",
      "val acc:  0.87625\n",
      "\n",
      "Epoch 9, Iteration 0, loss = 0.0339\n",
      "val acc:  0.88375\n",
      "\n",
      "Epoch 9, Iteration 50, loss = 0.0647\n",
      "val acc:  0.894375\n",
      "\n",
      "Epoch 10, Iteration 0, loss = 0.0456\n",
      "val acc:  0.88875\n",
      "\n",
      "Epoch 10, Iteration 50, loss = 0.0498\n",
      "val acc:  0.89375\n",
      "\n",
      "Epoch 11, Iteration 0, loss = 0.0493\n",
      "val acc:  0.885\n",
      "\n",
      "Epoch 11, Iteration 50, loss = 0.0499\n",
      "val acc:  0.893125\n",
      "\n",
      "Epoch 12, Iteration 0, loss = 0.0214\n",
      "val acc:  0.9025\n",
      "\n",
      "Epoch 12, Iteration 50, loss = 0.0518\n",
      "val acc:  0.903125\n",
      "\n",
      "Epoch 13, Iteration 0, loss = 0.0261\n",
      "val acc:  0.9075\n",
      "\n",
      "Epoch 13, Iteration 50, loss = 0.0614\n",
      "val acc:  0.905\n",
      "\n",
      "Epoch 14, Iteration 0, loss = 0.0329\n",
      "val acc:  0.891875\n",
      "\n",
      "Epoch 14, Iteration 50, loss = 0.0543\n",
      "val acc:  0.906875\n",
      "\n",
      "Epoch 15, Iteration 0, loss = 0.0215\n",
      "val acc:  0.909375\n",
      "\n",
      "Epoch 15, Iteration 50, loss = 0.0639\n",
      "val acc:  0.91375\n",
      "\n",
      "Epoch 16, Iteration 0, loss = 0.0836\n",
      "val acc:  0.9125\n",
      "\n",
      "Epoch 16, Iteration 50, loss = 0.0642\n",
      "val acc:  0.9125\n",
      "\n",
      "Epoch 17, Iteration 0, loss = 0.0271\n",
      "val acc:  0.90875\n",
      "\n",
      "Epoch 17, Iteration 50, loss = 0.0468\n",
      "val acc:  0.919375\n",
      "\n",
      "Epoch 18, Iteration 0, loss = 0.0884\n",
      "val acc:  0.91\n",
      "\n",
      "Epoch 18, Iteration 50, loss = 0.0369\n",
      "val acc:  0.91625\n",
      "\n",
      "Epoch 19, Iteration 0, loss = 0.0421\n",
      "val acc:  0.911875\n",
      "\n",
      "Epoch 19, Iteration 50, loss = 0.0967\n",
      "val acc:  0.91125\n",
      "\n",
      "Epoch 20, Iteration 0, loss = 0.0336\n",
      "val acc:  0.9125\n",
      "\n",
      "Epoch 20, Iteration 50, loss = 0.0287\n",
      "val acc:  0.91625\n",
      "\n",
      "Epoch 21, Iteration 0, loss = 0.0277\n",
      "val acc:  0.92\n",
      "\n",
      "Epoch 21, Iteration 50, loss = 0.0363\n",
      "val acc:  0.921875\n",
      "\n",
      "Epoch 22, Iteration 0, loss = 0.0269\n",
      "val acc:  0.91625\n",
      "\n",
      "Epoch 22, Iteration 50, loss = 0.0397\n",
      "val acc:  0.916875\n",
      "\n",
      "Epoch 23, Iteration 0, loss = 0.1416\n",
      "val acc:  0.909375\n",
      "\n",
      "Epoch 23, Iteration 50, loss = 0.0556\n",
      "val acc:  0.913125\n",
      "\n",
      "Epoch 24, Iteration 0, loss = 0.0212\n",
      "val acc:  0.905625\n",
      "\n",
      "Epoch 24, Iteration 50, loss = 0.0391\n",
      "val acc:  0.911875\n",
      "\n",
      "Epoch 25, Iteration 0, loss = 0.0195\n",
      "val acc:  0.92\n",
      "\n",
      "Epoch 25, Iteration 50, loss = 0.0548\n",
      "val acc:  0.916875\n",
      "\n",
      "Epoch 26, Iteration 0, loss = 0.1422\n",
      "val acc:  0.915625\n",
      "\n",
      "Epoch 26, Iteration 50, loss = 0.0374\n",
      "val acc:  0.915\n",
      "\n",
      "Epoch 27, Iteration 0, loss = 0.0167\n",
      "val acc:  0.915625\n",
      "\n",
      "Epoch 27, Iteration 50, loss = 0.0348\n",
      "val acc:  0.913125\n",
      "\n",
      "Epoch 28, Iteration 0, loss = 0.0232\n",
      "val acc:  0.9075\n",
      "\n",
      "Epoch 28, Iteration 50, loss = 0.0191\n",
      "val acc:  0.91875\n",
      "\n",
      "Epoch 29, Iteration 0, loss = 0.0157\n",
      "val acc:  0.904375\n",
      "\n",
      "Epoch 29, Iteration 50, loss = 0.0282\n",
      "val acc:  0.915\n",
      "\n",
      "Epoch 30, Iteration 0, loss = 0.0266\n",
      "val acc:  0.92\n",
      "\n",
      "Epoch 30, Iteration 50, loss = 0.0450\n",
      "val acc:  0.911875\n",
      "\n",
      "Epoch 31, Iteration 0, loss = 0.0248\n",
      "val acc:  0.9125\n",
      "\n",
      "Epoch 31, Iteration 50, loss = 0.0329\n",
      "val acc:  0.915625\n",
      "\n",
      "Epoch 32, Iteration 0, loss = 0.0289\n",
      "val acc:  0.89875\n",
      "\n",
      "Epoch 32, Iteration 50, loss = 0.0292\n",
      "val acc:  0.9225\n",
      "\n",
      "Epoch 33, Iteration 0, loss = 0.0287\n",
      "val acc:  0.911875\n",
      "\n",
      "Epoch 33, Iteration 50, loss = 0.0423\n",
      "val acc:  0.91625\n",
      "\n",
      "Epoch 34, Iteration 0, loss = 0.0128\n",
      "val acc:  0.919375\n",
      "\n",
      "Epoch 34, Iteration 50, loss = 0.0340\n",
      "val acc:  0.913125\n",
      "\n",
      "Epoch 35, Iteration 0, loss = 0.0151\n",
      "val acc:  0.91625\n",
      "\n",
      "Epoch 35, Iteration 50, loss = 0.0434\n",
      "val acc:  0.909375\n",
      "\n",
      "Epoch 36, Iteration 0, loss = 0.0111\n",
      "val acc:  0.923125\n",
      "\n",
      "Epoch 36, Iteration 50, loss = 0.0179\n",
      "val acc:  0.915\n",
      "\n",
      "Epoch 37, Iteration 0, loss = 0.0140\n",
      "val acc:  0.9025\n",
      "\n",
      "Epoch 37, Iteration 50, loss = 0.0302\n",
      "val acc:  0.92375\n",
      "\n",
      "Epoch 38, Iteration 0, loss = 0.0157\n",
      "val acc:  0.9275\n",
      "\n",
      "Epoch 38, Iteration 50, loss = 0.0434\n",
      "val acc:  0.916875\n",
      "\n",
      "Epoch 39, Iteration 0, loss = 0.0130\n",
      "val acc:  0.920625\n",
      "\n",
      "Epoch 39, Iteration 50, loss = 0.0340\n",
      "val acc:  0.920625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 3e-3\n",
    "wd = 5e-4\n",
    "\n",
    "textcnn_model = TextCNN(vocab, 100, max_word_size, 10)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(textcnn_model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "train(textcnn_model, loss_func, optimizer, loader_train, loader_val, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用全部数据训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0, loss = 2.3931\n",
      "\n",
      "Epoch 0, Iteration 50, loss = 2.1543\n",
      "\n",
      "Epoch 0, Iteration 100, loss = 1.4001\n",
      "\n",
      "Epoch 1, Iteration 0, loss = 1.0104\n",
      "\n",
      "Epoch 1, Iteration 50, loss = 0.8289\n",
      "\n",
      "Epoch 1, Iteration 100, loss = 0.6506\n",
      "\n",
      "Epoch 2, Iteration 0, loss = 0.3662\n",
      "\n",
      "Epoch 2, Iteration 50, loss = 0.4268\n",
      "\n",
      "Epoch 2, Iteration 100, loss = 0.4036\n",
      "\n",
      "Epoch 3, Iteration 0, loss = 0.2713\n",
      "\n",
      "Epoch 3, Iteration 50, loss = 0.1748\n",
      "\n",
      "Epoch 3, Iteration 100, loss = 0.2229\n",
      "\n",
      "Epoch 4, Iteration 0, loss = 0.1819\n",
      "\n",
      "Epoch 4, Iteration 50, loss = 0.1523\n",
      "\n",
      "Epoch 4, Iteration 100, loss = 0.2118\n",
      "\n",
      "Epoch 5, Iteration 0, loss = 0.1285\n",
      "\n",
      "Epoch 5, Iteration 50, loss = 0.1560\n",
      "\n",
      "Epoch 5, Iteration 100, loss = 0.1303\n",
      "\n",
      "Epoch 6, Iteration 0, loss = 0.1034\n",
      "\n",
      "Epoch 6, Iteration 50, loss = 0.0682\n",
      "\n",
      "Epoch 6, Iteration 100, loss = 0.1010\n",
      "\n",
      "Epoch 7, Iteration 0, loss = 0.0897\n",
      "\n",
      "Epoch 7, Iteration 50, loss = 0.0798\n",
      "\n",
      "Epoch 7, Iteration 100, loss = 0.1899\n",
      "\n",
      "Epoch 8, Iteration 0, loss = 0.0669\n",
      "\n",
      "Epoch 8, Iteration 50, loss = 0.1137\n",
      "\n",
      "Epoch 8, Iteration 100, loss = 0.0927\n",
      "\n",
      "Epoch 9, Iteration 0, loss = 0.0326\n",
      "\n",
      "Epoch 9, Iteration 50, loss = 0.0461\n",
      "\n",
      "Epoch 9, Iteration 100, loss = 0.0574\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_dataset = TensorDataset(\n",
    "    torch.LongTensor(all_data),\n",
    "    torch.LongTensor(all_label)\n",
    "    )\n",
    "loader_all = DataLoader(all_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "lr = 3e-3\n",
    "wd = 5e-4\n",
    "\n",
    "model = TextCNN(vocab, 100, max_word_size, 10)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "train(model, loss_func, optimizer, loader_all, [], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "test_label = []\n",
    "textcnn_model.eval()\n",
    "\n",
    "with open('./exp1data/test.txt') as testf:\n",
    "    testf.readline()\n",
    "    for line in testf.readlines():\n",
    "        id, text = line.split(',', 1)\n",
    "        \n",
    "        vec = np.zeros(max_word_size)\n",
    "        idx = 0\n",
    "        for token in nlp(text):\n",
    "            try:\n",
    "                vec[idx] = word2idx[token.lemma_]\n",
    "                idx += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        test_data.append(list(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HUAWEI\\AppData\\Local\\Temp\\ipykernel_8792\\3914925851.py:1: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  scores = textcnn_model(torch.LongTensor(test_data))\n"
     ]
    }
   ],
   "source": [
    "scores = textcnn_model(torch.LongTensor(test_data))\n",
    "\n",
    "for s in scores:\n",
    "    test_label.append(np.argmax(s.detach().numpy()))\n",
    "\n",
    "with open('./exp1data/textcnnoutput.txt', 'w') as outputf:\n",
    "    outputf.write('id, pred\\n')\n",
    "    for id, pred in enumerate(test_label):\n",
    "        outputf.write('%d, %d\\n' % (int(id), pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43d708b351378113bf2f787a7aa1b7d9095cd6b9d75b80a93905e682d7797b2c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('deepl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
