{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "PRINT = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(loader_val, model):\n",
    "    model.eval()\n",
    "    cor, all = 0, 0\n",
    "    for (x, y) in loader_val:\n",
    "        all += len(y)\n",
    "        scores = model(x)\n",
    "        for idx, each in enumerate(scores):\n",
    "            if y[idx] == np.argmax(each.detach().numpy()): \n",
    "                cor += 1\n",
    "\n",
    "    acc = cor / all\n",
    "    print('val acc: ', acc)\n",
    "\n",
    "def train(model, loss_func, optim, loader_train, loader_val, epoch=1):\n",
    "    for e in range(epoch):\n",
    "        for idx, (x, y) in enumerate(loader_train):\n",
    "            # switch to train mode\n",
    "            model.train()\n",
    "            scores = model(x)\n",
    "            loss = loss_func(scores, y)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            if idx % PRINT == 0:\n",
    "                print('Epoch %d, Iteration %d, loss = %.4f' % (e, idx, loss.item()))\n",
    "                if loader_val:\n",
    "                    val(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# countVectorizer = CountVectorizer(stop_words='english')\n",
    "tfidfVectorizer = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:00<00:00, 2123429.44it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('./exp1data/train_data.txt') as f:\n",
    "    train_data_raw = f.read()\n",
    "    f.close()\n",
    "\n",
    "train_data_raw = [json.loads(data) for data in train_data_raw.strip().split('\\n')]\n",
    "\n",
    "text_num = len(train_data_raw)\n",
    "texts, label = [], []\n",
    "\n",
    "for data in tqdm(train_data_raw):\n",
    "    texts.append(data['raw'])\n",
    "    label.append(data['label'])\n",
    "\n",
    "# matrix = countVectorizer.fit_transform(texts)\n",
    "# vocab = countVectorizer.get_feature_names_out()\n",
    "\n",
    "matrix = tfidfVectorizer.fit_transform(texts)\n",
    "vocab = tfidfVectorizer.get_feature_names_out()\n",
    "\n",
    "all_data = matrix.toarray()\n",
    "all_label = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, classes_num):\n",
    "        super(FC, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, classes_num)\n",
    "\n",
    "        # softmax将输出转为概率\n",
    "        self.sf = nn.Softmax(dim=1)\n",
    "    \n",
    "    # 不加激活默认RELU激活\n",
    "    def forward(self, x):\n",
    "        scores = self.fc1(x)\n",
    "        scores = self.fc2(scores)\n",
    "        return self.sf(scores)  # 加入softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0, loss = 2.3028\n",
      "\n",
      "Epoch 0, Iteration 50, loss = 2.2484\n",
      "\n",
      "Epoch 0, Iteration 100, loss = 2.0415\n",
      "\n",
      "Epoch 1, Iteration 0, loss = 1.7720\n",
      "\n",
      "Epoch 1, Iteration 50, loss = 1.6621\n",
      "\n",
      "Epoch 1, Iteration 100, loss = 1.5934\n",
      "\n",
      "Epoch 2, Iteration 0, loss = 1.5256\n",
      "\n",
      "Epoch 2, Iteration 50, loss = 1.5370\n",
      "\n",
      "Epoch 2, Iteration 100, loss = 1.5341\n",
      "\n",
      "Epoch 3, Iteration 0, loss = 1.4923\n",
      "\n",
      "Epoch 3, Iteration 50, loss = 1.5001\n",
      "\n",
      "Epoch 3, Iteration 100, loss = 1.5005\n",
      "\n",
      "Epoch 4, Iteration 0, loss = 1.4819\n",
      "\n",
      "Epoch 4, Iteration 50, loss = 1.4897\n",
      "\n",
      "Epoch 4, Iteration 100, loss = 1.5444\n",
      "\n",
      "Epoch 5, Iteration 0, loss = 1.4787\n",
      "\n",
      "Epoch 5, Iteration 50, loss = 1.5084\n",
      "\n",
      "Epoch 5, Iteration 100, loss = 1.5051\n",
      "\n",
      "Epoch 6, Iteration 0, loss = 1.4766\n",
      "\n",
      "Epoch 6, Iteration 50, loss = 1.4788\n",
      "\n",
      "Epoch 6, Iteration 100, loss = 1.4878\n",
      "\n",
      "Epoch 7, Iteration 0, loss = 1.4776\n",
      "\n",
      "Epoch 7, Iteration 50, loss = 1.4798\n",
      "\n",
      "Epoch 7, Iteration 100, loss = 1.4962\n",
      "\n",
      "Epoch 8, Iteration 0, loss = 1.4732\n",
      "\n",
      "Epoch 8, Iteration 50, loss = 1.4855\n",
      "\n",
      "Epoch 8, Iteration 100, loss = 1.4891\n",
      "\n",
      "Epoch 9, Iteration 0, loss = 1.4706\n",
      "\n",
      "Epoch 9, Iteration 50, loss = 1.4756\n",
      "\n",
      "Epoch 9, Iteration 100, loss = 1.4826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_dataset = TensorDataset(\n",
    "    torch.FloatTensor(all_data),\n",
    "    torch.LongTensor(all_label)\n",
    "    )\n",
    "loader_all = DataLoader(all_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "lr = 9e-4\n",
    "wd = 1e-4\n",
    "\n",
    "mlp_model = FC(all_data.shape[1], int(np.sqrt(all_data.shape[1])), 10)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "train(mlp_model, loss_func, optimizer, loader_all, [], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [05:06<00:00, 26.12it/s]\n",
      "100%|██████████| 8000/8000 [00:00<00:00, 8118.83it/s]\n",
      "100%|██████████| 8000/8000 [00:01<00:00, 5474.27it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('./exp1data/train_data.txt') as f:\n",
    "    train_data_raw = f.read()\n",
    "    f.close()\n",
    "\n",
    "train_data_raw = [json.loads(data) for data in train_data_raw.strip().split('\\n')]\n",
    "\n",
    "word_set = set()\n",
    "for data in tqdm(train_data_raw):\n",
    "    data['nlp']=nlp(data['raw'])\n",
    "    for token in data['nlp']:\n",
    "        if (not token.is_stop) and (not token.is_punct):\n",
    "            word_set.add(token.lemma_)\n",
    "\n",
    "vocab = list(word_set)\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "max_word_size = 0\n",
    "for data in tqdm(train_data_raw):\n",
    "    temp_size = 0\n",
    "    for token in data['nlp']:\n",
    "        if token.lemma_ in word_set:\n",
    "            temp_size += 1\n",
    "    if temp_size > max_word_size:\n",
    "        max_word_size = temp_size\n",
    "\n",
    "for data in tqdm(train_data_raw):\n",
    "    vec = np.zeros(max_word_size)\n",
    "    idx = 0\n",
    "    for token in data['nlp']:\n",
    "        try:\n",
    "            vec[idx] = word2idx[token.lemma_]\n",
    "            idx += 1\n",
    "        except:\n",
    "            pass\n",
    "    data['vec'] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:00<00:00, 64317.12it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "train_label = []\n",
    "\n",
    "for data in tqdm(train_data_raw):\n",
    "    train_data.append(list(data['vec']))\n",
    "    train_label.append(data['label'])\n",
    "\n",
    "all_data = np.array(train_data)\n",
    "all_label = np.array(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab, embedding_size, max_word, class_num):\n",
    "        super(TextCNN, self).__init__()\n",
    "\n",
    "        # Embedding\n",
    "        self.ebd = nn.Embedding(len(vocab), embedding_size)\n",
    "\n",
    "        # CNN\n",
    "        output_channel = 100          # 这里设置多通道防止过拟合，但是在原论文的实际实验中效果不大\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, \n",
    "                out_channels=output_channel, \n",
    "                kernel_size=(2, embedding_size), \n",
    "                stride=1\n",
    "                ),                  # 经过这一卷积层后feature map大小变为(maxword-1) * embedding_size\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),   # 经过maxpooling后变为(maxword-1)/2 * embedding_size\n",
    "            nn.Dropout(0.5)         # 同样为了防止过拟合\n",
    "        )\n",
    "\n",
    "        # Fully Connected\n",
    "        self.fc = nn.Linear(int((max_word - 1) / 2) * output_channel, class_num)\n",
    "\n",
    "        # softmax 将输出转为概率\n",
    "        self.sf = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.ebd(x).unsqueeze(1)\n",
    "        conved = self.cnn(embedding)\n",
    "        flatten = conved.view(x.shape[0], -1)\n",
    "        output = self.fc(flatten) \n",
    "        return self.sf(output) # 加入softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0, loss = 2.3016\n",
      "\n",
      "Epoch 0, Iteration 50, loss = 2.3362\n",
      "\n",
      "Epoch 0, Iteration 100, loss = 2.3205\n",
      "\n",
      "Epoch 1, Iteration 0, loss = 2.2737\n",
      "\n",
      "Epoch 1, Iteration 50, loss = 2.3049\n",
      "\n",
      "Epoch 1, Iteration 100, loss = 2.3242\n",
      "\n",
      "Epoch 2, Iteration 0, loss = 2.2827\n",
      "\n",
      "Epoch 2, Iteration 50, loss = 2.1090\n",
      "\n",
      "Epoch 2, Iteration 100, loss = 2.0623\n",
      "\n",
      "Epoch 3, Iteration 0, loss = 2.0638\n",
      "\n",
      "Epoch 3, Iteration 50, loss = 2.0732\n",
      "\n",
      "Epoch 3, Iteration 100, loss = 1.9181\n",
      "\n",
      "Epoch 4, Iteration 0, loss = 1.7971\n",
      "\n",
      "Epoch 4, Iteration 50, loss = 1.8716\n",
      "\n",
      "Epoch 4, Iteration 100, loss = 1.7549\n",
      "\n",
      "Epoch 5, Iteration 0, loss = 1.6283\n",
      "\n",
      "Epoch 5, Iteration 50, loss = 1.6055\n",
      "\n",
      "Epoch 5, Iteration 100, loss = 1.6427\n",
      "\n",
      "Epoch 6, Iteration 0, loss = 1.6619\n",
      "\n",
      "Epoch 6, Iteration 50, loss = 1.5553\n",
      "\n",
      "Epoch 6, Iteration 100, loss = 1.5630\n",
      "\n",
      "Epoch 7, Iteration 0, loss = 1.5397\n",
      "\n",
      "Epoch 7, Iteration 50, loss = 1.5666\n",
      "\n",
      "Epoch 7, Iteration 100, loss = 1.6229\n",
      "\n",
      "Epoch 8, Iteration 0, loss = 1.5494\n",
      "\n",
      "Epoch 8, Iteration 50, loss = 1.5157\n",
      "\n",
      "Epoch 8, Iteration 100, loss = 1.5275\n",
      "\n",
      "Epoch 9, Iteration 0, loss = 1.4964\n",
      "\n",
      "Epoch 9, Iteration 50, loss = 1.5548\n",
      "\n",
      "Epoch 9, Iteration 100, loss = 1.5466\n",
      "\n",
      "Epoch 10, Iteration 0, loss = 1.4910\n",
      "\n",
      "Epoch 10, Iteration 50, loss = 1.4911\n",
      "\n",
      "Epoch 10, Iteration 100, loss = 1.5315\n",
      "\n",
      "Epoch 11, Iteration 0, loss = 1.5098\n",
      "\n",
      "Epoch 11, Iteration 50, loss = 1.5112\n",
      "\n",
      "Epoch 11, Iteration 100, loss = 1.5102\n",
      "\n",
      "Epoch 12, Iteration 0, loss = 1.4747\n",
      "\n",
      "Epoch 12, Iteration 50, loss = 1.5096\n",
      "\n",
      "Epoch 12, Iteration 100, loss = 1.5152\n",
      "\n",
      "Epoch 13, Iteration 0, loss = 1.4972\n",
      "\n",
      "Epoch 13, Iteration 50, loss = 1.5099\n",
      "\n",
      "Epoch 13, Iteration 100, loss = 1.5040\n",
      "\n",
      "Epoch 14, Iteration 0, loss = 1.5034\n",
      "\n",
      "Epoch 14, Iteration 50, loss = 1.5152\n",
      "\n",
      "Epoch 14, Iteration 100, loss = 1.5201\n",
      "\n",
      "Epoch 15, Iteration 0, loss = 1.5120\n",
      "\n",
      "Epoch 15, Iteration 50, loss = 1.5245\n",
      "\n",
      "Epoch 15, Iteration 100, loss = 1.5482\n",
      "\n",
      "Epoch 16, Iteration 0, loss = 1.4699\n",
      "\n",
      "Epoch 16, Iteration 50, loss = 1.5019\n",
      "\n",
      "Epoch 16, Iteration 100, loss = 1.5176\n",
      "\n",
      "Epoch 17, Iteration 0, loss = 1.5126\n",
      "\n",
      "Epoch 17, Iteration 50, loss = 1.5040\n",
      "\n",
      "Epoch 17, Iteration 100, loss = 1.5114\n",
      "\n",
      "Epoch 18, Iteration 0, loss = 1.4816\n",
      "\n",
      "Epoch 18, Iteration 50, loss = 1.5028\n",
      "\n",
      "Epoch 18, Iteration 100, loss = 1.5276\n",
      "\n",
      "Epoch 19, Iteration 0, loss = 1.4809\n",
      "\n",
      "Epoch 19, Iteration 50, loss = 1.5517\n",
      "\n",
      "Epoch 19, Iteration 100, loss = 1.5268\n",
      "\n",
      "Epoch 20, Iteration 0, loss = 1.5102\n",
      "\n",
      "Epoch 20, Iteration 50, loss = 1.4917\n",
      "\n",
      "Epoch 20, Iteration 100, loss = 1.5396\n",
      "\n",
      "Epoch 21, Iteration 0, loss = 1.4794\n",
      "\n",
      "Epoch 21, Iteration 50, loss = 1.5166\n",
      "\n",
      "Epoch 21, Iteration 100, loss = 1.5401\n",
      "\n",
      "Epoch 22, Iteration 0, loss = 1.4804\n",
      "\n",
      "Epoch 22, Iteration 50, loss = 1.5041\n",
      "\n",
      "Epoch 22, Iteration 100, loss = 1.5267\n",
      "\n",
      "Epoch 23, Iteration 0, loss = 1.5054\n",
      "\n",
      "Epoch 23, Iteration 50, loss = 1.4992\n",
      "\n",
      "Epoch 23, Iteration 100, loss = 1.4986\n",
      "\n",
      "Epoch 24, Iteration 0, loss = 1.4706\n",
      "\n",
      "Epoch 24, Iteration 50, loss = 1.5015\n",
      "\n",
      "Epoch 24, Iteration 100, loss = 1.5099\n",
      "\n",
      "Epoch 25, Iteration 0, loss = 1.4967\n",
      "\n",
      "Epoch 25, Iteration 50, loss = 1.5332\n",
      "\n",
      "Epoch 25, Iteration 100, loss = 1.5221\n",
      "\n",
      "Epoch 26, Iteration 0, loss = 1.5069\n",
      "\n",
      "Epoch 26, Iteration 50, loss = 1.5343\n",
      "\n",
      "Epoch 26, Iteration 100, loss = 1.5198\n",
      "\n",
      "Epoch 27, Iteration 0, loss = 1.4739\n",
      "\n",
      "Epoch 27, Iteration 50, loss = 1.4901\n",
      "\n",
      "Epoch 27, Iteration 100, loss = 1.4940\n",
      "\n",
      "Epoch 28, Iteration 0, loss = 1.4737\n",
      "\n",
      "Epoch 28, Iteration 50, loss = 1.4779\n",
      "\n",
      "Epoch 28, Iteration 100, loss = 1.4941\n",
      "\n",
      "Epoch 29, Iteration 0, loss = 1.4847\n",
      "\n",
      "Epoch 29, Iteration 50, loss = 1.5155\n",
      "\n",
      "Epoch 29, Iteration 100, loss = 1.5158\n",
      "\n",
      "Epoch 30, Iteration 0, loss = 1.4926\n",
      "\n",
      "Epoch 30, Iteration 50, loss = 1.5067\n",
      "\n",
      "Epoch 30, Iteration 100, loss = 1.5222\n",
      "\n",
      "Epoch 31, Iteration 0, loss = 1.4749\n",
      "\n",
      "Epoch 31, Iteration 50, loss = 1.4769\n",
      "\n",
      "Epoch 31, Iteration 100, loss = 1.4913\n",
      "\n",
      "Epoch 32, Iteration 0, loss = 1.4824\n",
      "\n",
      "Epoch 32, Iteration 50, loss = 1.4888\n",
      "\n",
      "Epoch 32, Iteration 100, loss = 1.5416\n",
      "\n",
      "Epoch 33, Iteration 0, loss = 1.5085\n",
      "\n",
      "Epoch 33, Iteration 50, loss = 1.5195\n",
      "\n",
      "Epoch 33, Iteration 100, loss = 1.5042\n",
      "\n",
      "Epoch 34, Iteration 0, loss = 1.4985\n",
      "\n",
      "Epoch 34, Iteration 50, loss = 1.4851\n",
      "\n",
      "Epoch 34, Iteration 100, loss = 1.5240\n",
      "\n",
      "Epoch 35, Iteration 0, loss = 1.5050\n",
      "\n",
      "Epoch 35, Iteration 50, loss = 1.4896\n",
      "\n",
      "Epoch 35, Iteration 100, loss = 1.4793\n",
      "\n",
      "Epoch 36, Iteration 0, loss = 1.4973\n",
      "\n",
      "Epoch 36, Iteration 50, loss = 1.4955\n",
      "\n",
      "Epoch 36, Iteration 100, loss = 1.5160\n",
      "\n",
      "Epoch 37, Iteration 0, loss = 1.4682\n",
      "\n",
      "Epoch 37, Iteration 50, loss = 1.4748\n",
      "\n",
      "Epoch 37, Iteration 100, loss = 1.4889\n",
      "\n",
      "Epoch 38, Iteration 0, loss = 1.4773\n",
      "\n",
      "Epoch 38, Iteration 50, loss = 1.4870\n",
      "\n",
      "Epoch 38, Iteration 100, loss = 1.4968\n",
      "\n",
      "Epoch 39, Iteration 0, loss = 1.4809\n",
      "\n",
      "Epoch 39, Iteration 50, loss = 1.4717\n",
      "\n",
      "Epoch 39, Iteration 100, loss = 1.5005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_dataset = TensorDataset(\n",
    "    torch.LongTensor(all_data),\n",
    "    torch.LongTensor(all_label)\n",
    "    )\n",
    "loader_all = DataLoader(all_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "lr = 3e-3\n",
    "wd = 5e-4\n",
    "\n",
    "textcnn_model = TextCNN(vocab, 100, max_word_size, 10)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(textcnn_model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "train(textcnn_model, loss_func, optimizer, loader_all, [], 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_test_data = []\n",
    "textcnn_test_data = []\n",
    "\n",
    "test_label = []\n",
    "mlp_model.eval()\n",
    "textcnn_model.eval()\n",
    "\n",
    "with open('./exp1data/test.txt') as testf:\n",
    "    testf.readline()\n",
    "    for line in testf.readlines():\n",
    "        id, text = line.split(',', 1)\n",
    "\n",
    "        mlp_test_data.append(text)\n",
    "        \n",
    "        vec = np.zeros(max_word_size)\n",
    "        idx = 0\n",
    "        for token in nlp(text):\n",
    "            try:\n",
    "                vec[idx] = word2idx[token.lemma_]\n",
    "                idx += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        textcnn_test_data.append(list(vec))\n",
    "\n",
    "    mlp_test_data = tfidfVectorizer.transform(mlp_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HUAWEI\\AppData\\Local\\Temp\\ipykernel_1324\\3393184021.py:2: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  textcnn_scores = textcnn_model(torch.LongTensor(textcnn_test_data))\n"
     ]
    }
   ],
   "source": [
    "mlp_scores = mlp_model(torch.FloatTensor(mlp_test_data.toarray()))\n",
    "textcnn_scores = textcnn_model(torch.LongTensor(textcnn_test_data))\n",
    "\n",
    "for s1, s2 in zip(mlp_scores, textcnn_scores):\n",
    "    s = s1 + s2\n",
    "    test_label.append(np.argmax(s.detach().numpy()))\n",
    "\n",
    "with open('./exp1data/ensembleoutput.txt', 'w') as outputf:\n",
    "    outputf.write('id, pred\\n')\n",
    "    for id, pred in enumerate(test_label):\n",
    "        outputf.write('%d, %d\\n' % (int(id), pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43d708b351378113bf2f787a7aa1b7d9095cd6b9d75b80a93905e682d7797b2c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('deepl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
